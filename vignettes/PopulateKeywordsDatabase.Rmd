---
title: "Populate Keywords Database"
author: "Anthony Arroyo, Josh Forster"
date: "10/22/2022"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
library(magrittr)
library(jsonlite)
library(here)
source(here::here("R/generateKeywordsDatasets.R"))
```

# Datasets

We pulled a variety of job descriptions regarding data science jobs.
Shown below are some the tweakable parameters to create the database.

```{r parameters}
postingsJson = "data/job_description_data.json"
dontWrite = TRUE

job = jsonlite::read_json(here::here(postingsJson))
jobIdVector = sapply(job, function(x) { x$link }) %>%
  magrittr::set_names(paste0("linkedin_", 1:length(.)), .)
writeFiles = rep("data/keywords_linkedin/", length(jobIdVector))

read.csv(here::here("data/keyword_posting_crosswalk.csv")) %>% 
  rbind(., data.frame(job_id = jobIdVector, file_store = writeFiles, job_store = postingsJson, job_url = names(jobIdVector))) %>%
    .[! duplicated(.), ] %>%
      write.csv(., here::here("data/keyword_posting_crosswalk.csv"), row.names = FALSE)


if (dontWrite) {
  writeFiles = NA
} else {
  writeFiles = here::here(writeFiles)
}

head(jobIdVector, 2)
```

# Captures from the English Dictionary

To determine our stop words, we used Webster's dictionary to select nouns, verbs, and adjectives.

```{r defaults}
captureGroups = c("n.", "a.", "v.")
dictionary = here::here("data/dictionary.json") %>%
  readLines(.) %>%
    jsonlite::fromJSON(.)

head(dictionary[-1:-705, -3])
```

# Finished Aggregates

The data created shows the keyphrase, number of occurences across all inputs, and number of words.
We can use this to determine the most common keyphrases in Data Science job descriptions.

```{r runner}
values = GenerateKeywords(job, jobIdVector, writeFiles, dictionary, captureGroups, GrabLinkedin)
aggregates = SumFreq(values)
singularAggregates = aggregates %>% .[.$numWords == 1, ]
writeReports = FALSE

if (writeReports) {
  write.csv(aggregates, here::here("data/outputs/aggregateLinkedinPhrases.csv"), row.names = F)
  write.csv(singularAggregates, here::here("data/outputs/aggregateLinkedinKeywords.csv"), row.names = F)
}

head(aggregates[-1:-10, ], 20)
```

# Combining all data points

If we prefer to look at the data in summary as opposed to by source, that is available too.
By providing each data folder, we are loading our database and can select all available sets.
Furthermore, the available sets are made known by reading the crosswalk table containing the primary keys for the interop.

```{r amalgamate}
allValues = here::here("data/keyword_posting_crosswalk.csv") %>%
  read.csv(.) %>% 
    .$file_store %>%
      unique(.) %>%
        here::here(.) %>%
          lapply(., LoadKeywordDatabase) %>%
            do.call(c, .)

allAggregates = SumFreq(allValues)
allSingularAggregates = allAggregates %>% .[.$numWords == 1, ]
writeFinalReports = FALSE

if (writeFinalReports) {
  write.csv(allAggregates, here::here("data/outputs/aggregateAllPhrases.csv"), row.names = F)
  write.csv(allSingularAggregates, here::here("data/outputs/aggregateAllKeywords.csv"), row.names = F)
}

if (tools::md5sum(here::here("data/outputs/aggregateAllPhrases.csv")) == "224a1868ac42be8e8390d273f142b66a") {
  allAggregates[c(16, 18, 21, 27, 31, 34, 42, 46, 50, 53, 54, 67, 115, 140, 178, 179), ]
} else {
  head(allAggregates[-1:-15, ], 20)
}
```


