{"body":{"div":{"text":["          "],"h2":{"text":["            About the job          "],".attrs":["mt5 t-20 t-bold mb4"]},"comment":{},"comment.1":{},"text.1":["        "],"span":{"text":["                "],"p":{"strong":{"u":["About the Compay"]}},"p.1":["Gravitate AI is a US-based professional services company that provides AI architect services and develops data-driven product solutions for start-ups. Our global and diverse team represents different countries and cultures and is composed of talented data scientists, data engineers, and software developers that specialize in Natural Language Processing, Image Recognition, segmentation, and classification models and tools. Our wide range of applications using AI and ML include, but are not limited, to marketing analysis, medical applications, data pipelines, and microservice APIs."],"p.2":{"br":{}},"p.3":{"strong":{"u":["About the Role"]}},"p.4":{"text":["Our Data Scientists and Data Engineers are tasked with data extraction, web scraping, and sourcing of various data APIs, as well as application data processing, data transfer, data storage, and database operations. This is a key role to ensure a well-established data pipeline and data system for our clients who are accelerating product development. This role provides a solid foundation for careers in AI and ML, as well as working with innovative early-stage companies, improving their cutting-edge technologies and providing them with a unique and valuable opportunity to grow to make a lasting impact! "],"strong":{"i":["*Note this is a contractor role which may be hourly, part-time or full-time depending on the company's needs at the time of hiring*"]}},"p.5":{"br":{}},"p.6":{"strong":{"u":["Responsibilities"]}},"p.7":["Help build end-to-end modern data pipelines using different types of data sources including images, text, web data, and structured data, involving all different stages of the data lifecycle. There will be opportunities to learn and perform other ML tasks, such as implementing AI and ML algorithms using microservice APIs"],"p.8":{"br":{}},"p.9":["Key responsibilities include:"],"ul":{"li":["Data extraction, including web scraping and sourcing different data APIs."],"li.1":["Scaling process steps for a variety of different large data sources."],"li.2":["Working with teams to automate and scale different steps in data pipelines using popular tools like Airflow, Kafka, Docker, etc."],"li.3":["Assisting with deployment of AI-based microservices APIs and software."],"li.4":["Hands-on technical contributions to data processing and database tasks with internal and client teams through cross-functional collaboration."]},"p.10":{"br":{}},"p.11":{"strong":{"u":["Qualifications"]}},"ul.1":{"li":["Bachelor's degree in Computer Science, Mathematics, Statistics, Physics, or other quantitative major."],"li.1":["1-3 years of relevant work experience."],"li.2":["Proficient in Python and at least one other programming language."],"li.3":["Experienced Linux user. "],"li.4":["Experience with web scraping and working with data APIs. "],"li.5":["Familiarity with database operations"]},"comment":{}},"text.2":["      "],".attrs":["jobs-box__html-content jobs-description-content__text t-14 t-normal          jobs-description-content__text--stretch","job-details","-1","true"]},"comment":{},"text":["      "],"div.1":{"comment":{},"text":["      "],".attrs":["jobs-description__details"]},"text.1":["          "]}}
