{"body":{"div":{"text":["          "],"h2":{"text":["            About the job          "],".attrs":["mt5 t-20 t-bold mb4"]},"comment":{},"comment.1":{},"text.1":["        "],"span":{"text":["                "],"p":{"text":["This position is ideal for an experienced engineer with a strong Python, Spark, and SQL experience who is looking to work with massive amounts of data in a cutting-edge, cloud-based environment. You will be responsible for building/maintaining/expanding data solutions using tools from Apache and AWS, including Redshift, Spark, S3, and Airflow."],"br":{},"br.1":{}},"br":{},"p.1":{"br":{},"br.1":{},"br.2":{}},"br.1":{},"p.2":{"text":["This role is open to remote for the right candidate, preferably within the EST Timezone. Based in NY is a plus."],"br":{},"br.1":{}},"br.2":{},"p.3":{"br":{},"br.1":{}},"br.3":{},"p.4":{"strong":{"text":["Responsibilities"],"br":{},"br.1":{}}},"br.4":{},"ul":{"li":["Using various tools such as Python, Spark, and SQL to create data pipelines to extract, cleanse, and integrate data from a variety of sources and formats"],"text":[" "],"li.1":["Develop scalable and re-usable solutions that support both real-time and/or batch-based data processing"],"text.1":[" "],"li.2":["Own the data quality for the integration pipelines"],"text.2":[" "],"li.3":["Fulfill ad-hoc requests from business partners for data residing in Redshift and/or S3"],"text.3":[" "],"li.4":["Interact with client-facing teams to lead technical solution discussions"],"text.4":[" "],"li.5":["Experience with Agile/Scrum methodology framework for product development"],"text.5":[" "],"li.6":["Maintain/evolve Docker container deployments for development and production environments"],"text.6":[" "],"li.7":["Maintain/evolve AWS Redshift cluster"],"text.7":[" "]},"br.5":{},"p.5":{"strong":{"text":["Qualifications"],"br":{},"br.1":{},"br.2":{}}},"br.6":{},"ul.1":{"li":["2+ years of programming experience with Python"],"text":[" "],"li.1":["2+ years of experience in SQL, preferably Postgres"],"text.1":[" "],"li.2":["2+ years experience working with AWS, particularly Redshift, S3, and EC2"],"text.2":[" "],"li.3":["Experience building/maintaining Spark based data pipelines"],"text.3":[" "],"li.4":["Experience building/maintaining Docker containers"],"text.4":[" "]},"br.7":{},"p.6":{"strong":{"text":["Recommended Skills"],"br":{},"br.1":{},"br.2":{}}},"br.8":{},"ul.2":{"li":["Hands-on experience with AWS ecosystem of tools and technologies"],"text":[" "],"li.1":["Ability to build/maintain ETL pipelines"],"text.1":[" "],"li.2":["Experience with complex multi-server environments and high availability environments"],"text.2":[" "],"li.3":["Experience with system monitoring, log management, and error notification"],"text.3":[" "],"li.4":["Experience in Ad-Tech industry a plus"],"text.4":[" "]},"br.9":{},"p.7":{"br":{},"br.1":{},"br.2":{}},"br.10":{},"p.8":{"text":["We offer full health coverage, generous PTO, great office culture."],"br":{},"br.1":{}},"br.11":{},"p.9":["We are an Equal Opportunity Employer and seek to foster community, inclusion and diversity within the organization. We encourage all qualified candidates, regardless of racial, religious, sexual or gender identity, to apply."],"comment":{},"text.1":["        "]},"text.2":["      "],".attrs":["jobs-box__html-content jobs-description-content__text t-14 t-normal          jobs-description-content__text--stretch","job-details","-1","true"]},"comment":{},"text":["      "],"div.1":{"comment":{},"text":["      "],".attrs":["jobs-description__details"]},"text.1":["          "]}}
